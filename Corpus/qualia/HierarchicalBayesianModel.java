// SPDX-License-Identifier: GPL-3.0-only
// SPDX-FileCopyrightText: 2025 Jumping Quail Solutions
package qualia;

import java.util.ArrayList;
import java.util.List;
import java.util.Objects;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.ThreadLocalRandom;
import java.security.SecureRandom;

/**
 * Core model implementation for Hierarchical Bayesian inference.
 *
 * <p>Defines Ψ using the affine+exponential+cap structure:
 * O(α) = α·S + (1−α)·N,
 * pen = exp(−[λ1·R_a + λ2·R_v]),
 * P(H|E, β) = min{β·P(H|E), 1},
 * Ψ = O · pen · P(H|E, β).
 *
 * <p>Thread-safety: stateless aside from immutable {@link ModelPriors}; safe for
 * concurrent use.
 */
public final class HierarchicalBayesianModel {
    private final ModelPriors priors;

    /**
     * Constructs a model with default priors.
     */
    public HierarchicalBayesianModel() {
        this(ModelPriors.defaults());
    }

    /**
     * Constructs a model with explicit priors.
     * @param priors hyperparameters (non-null)
     */
    public HierarchicalBayesianModel(ModelPriors priors) {
        this.priors = Objects.requireNonNull(priors, "priors must not be null");
    }

    /**
     * Calculates Ψ(x) for a given claim and parameter sample.
     * Result is clamped to [0,1] for numerical stability.
     *
     * @param claim observed claim data (non-null)
     * @param params parameter draw (non-null)
     * @return Ψ in [0,1]
     */
    public double calculatePsi(ClaimData claim, ModelParameters params) {
        Objects.requireNonNull(claim, "claim must not be null");
        Objects.requireNonNull(params, "params must not be null");

        double O = params.alpha() * params.S() + (1.0 - params.alpha()) * params.N();
        double penaltyExponent = -(
                priors.lambda1() * claim.riskAuthenticity() +
                priors.lambda2() * claim.riskVirality()
        );
        double pen = Math.exp(penaltyExponent);
        double p_H_given_E_beta = Math.min(params.beta() * claim.probabilityHgivenE(), 1.0);
        double psi = O * pen * p_H_given_E_beta;
        if (psi < 0.0) return 0.0;
        if (psi > 1.0) return 1.0;
        return psi;
    }

    /**
     * log-likelihood for a single observation: y | Ψ ~ Bernoulli(Ψ).
     *
     * @param claim observation
     * @param params parameter draw
     * @return log p(y|Ψ)
     */
    public double logLikelihood(ClaimData claim, ModelParameters params) {
        double psi = calculatePsi(claim, params);
        double epsilon = 1e-9;
        double clamped = Math.max(epsilon, Math.min(1.0 - epsilon, psi));
        return claim.isVerifiedTrue() ? Math.log(clamped) : Math.log(1.0 - clamped);
    }

    /**
     * Sum log-likelihood over a dataset.
     *
     * @param dataset list of observations
     * @param params parameter draw
     * @return Σ log p(y_i|Ψ_i)
     */
    public double totalLogLikelihood(List<ClaimData> dataset, ModelParameters params) {
        Objects.requireNonNull(dataset, "dataset must not be null");
        return dataset.stream()
                .mapToDouble(claim -> logLikelihood(claim, params))
                .sum();
    }

    /**
     * Log-priors for a given parameter draw. Placeholder (returns 0.0).
     * In a full implementation, provide log-PDFs for Beta, LogNormal, etc.
     *
     * @param params parameter draw
     * @return log p(params)
     */
    public double logPriors(ModelParameters params) {
        double lpS = Stats.logBetaPdf(params.S(), priors.s_alpha(), priors.s_beta());
        double lpN = Stats.logBetaPdf(params.N(), priors.n_alpha(), priors.n_beta());
        double lpAlpha = Stats.logBetaPdf(params.alpha(), priors.alpha_alpha(), priors.alpha_beta());
        double lpBeta = Stats.logLogNormalPdf(params.beta(), priors.beta_mu(), priors.beta_sigma());
        return lpS + lpN + lpAlpha + lpBeta;
    }

    /**
     * Log-posterior up to an additive constant: logLik + logPrior.
     *
     * @param dataset observations
     * @param params parameter draw
     * @return log p(params|data) + C
     */
    public double logPosterior(List<ClaimData> dataset, ModelParameters params) {
        return totalLogLikelihood(dataset, params) + logPriors(params);
    }

    /**
     * Posterior inference via a simple Random-Walk Metropolis-Hastings prototype.
     *
     * <p>This is a minimal, dependency-free scaffold useful for tests and
     * baselines. For production use, prefer HMC/NUTS via a dedicated library.
     */
    public List<ModelParameters> performInference(List<ClaimData> dataset, int sampleCount) {
        long seed = ThreadLocalRandom.current().nextLong();
        System.out.println("performInference: seed=" + seed + ", samples=" + sampleCount);
        return runMhChain(dataset, sampleCount, seed);
    }

    /**
     * Overload allowing explicit seed control for reproducibility.
     */
    public List<ModelParameters> performInference(List<ClaimData> dataset, int sampleCount, long seed) {
        System.out.println("performInference(explicit): seed=" + seed + ", samples=" + sampleCount);
        return runMhChain(dataset, sampleCount, seed);
    }

    /**
     * Runs multiple independent MH chains in parallel and returns per-chain samples.
     */
    public List<List<ModelParameters>> performInferenceParallel(List<ClaimData> dataset, int chains, int samplesPerChain) {
        if (chains <= 0 || samplesPerChain <= 0) return List.of();
        // Choose a cryptographically-strong base seed to reduce collision risk across invocations
        long baseSeed = new SecureRandom().nextLong();
        System.out.println("performInferenceParallel: baseSeed=" + baseSeed + ", chains=" + chains + ", samplesPerChain=" + samplesPerChain);
        return performInferenceParallel(dataset, chains, samplesPerChain, baseSeed);
    }

    /**
     * Overload allowing an explicit baseSeed. Per-chain seeds are derived as
     * baseSeed + c * PHI64 where PHI64 is a large odd constant to avoid collisions.
     */
    public List<List<ModelParameters>> performInferenceParallel(List<ClaimData> dataset, int chains, int samplesPerChain, long baseSeed) {
        if (chains <= 0 || samplesPerChain <= 0) return List.of();
        int poolSize = Math.min(chains, Math.max(1, Runtime.getRuntime().availableProcessors()));
        ExecutorService pool = Executors.newFixedThreadPool(poolSize, r -> {
            Thread t = new Thread(r, "qualia-mh-");
            t.setDaemon(true);
            return t;
        });

        try {
            List<CompletableFuture<List<ModelParameters>>> futures = new ArrayList<>(chains);
            for (int c = 0; c < chains; c++) {
                // Use the 64-bit golden ratio odd constant for good spacing in Z/2^64Z
                final long PHI64 = 0x9E3779B97F4A7C15L;
                final long seed = baseSeed + (PHI64 * (long) c);
                futures.add(CompletableFuture.supplyAsync(() -> runMhChain(dataset, samplesPerChain, seed), pool));
            }
            List<List<ModelParameters>> results = new ArrayList<>(chains);
            for (CompletableFuture<List<ModelParameters>> f : futures) {
                results.add(f.join());
            }
            return results;
        } finally {
            pool.shutdown();
            try { pool.awaitTermination(5, TimeUnit.SECONDS); } catch (InterruptedException ignored) { Thread.currentThread().interrupt(); }
        }
    }

    /**
     * Convenience wrapper to compute convergence diagnostics from chains.
     */
    public Diagnostics diagnose(List<List<ModelParameters>> chains) {
        return Diagnostics.fromChains(chains);
    }

    private List<ModelParameters> runMhChain(List<ClaimData> dataset, int sampleCount, long seed) {
        System.out.println("Starting MH chain (seed=" + seed + ")...");
        if (sampleCount <= 0) return List.of();

        ModelParameters current = new ModelParameters(0.7, 0.6, 0.5, 1.0);
        double currentLogPost = logPosterior(dataset, current);
        ArrayList<ModelParameters> samples = new ArrayList<>(sampleCount);

        java.util.Random rng = new java.util.Random(seed);
        double stepS = 0.05, stepN = 0.05, stepAlpha = 0.05, stepBeta = 0.10;

        int burn = Math.min(500, Math.max(0, sampleCount / 5));
        int thin = Math.max(1, sampleCount / Math.max(1, Math.min(100, sampleCount)));
        int kept = 0;
        for (int iter = 0; kept < sampleCount; ) {
            double propS = clamp01(current.S() + stepS * (rng.nextDouble() * 2 - 1));
            double propN = clamp01(current.N() + stepN * (rng.nextDouble() * 2 - 1));
            double propAlpha = clamp01(current.alpha() + stepAlpha * (rng.nextDouble() * 2 - 1));
            double propBeta = Math.max(1e-6, current.beta() * Math.exp(stepBeta * (rng.nextDouble() * 2 - 1)));

            ModelParameters proposal = new ModelParameters(propS, propN, propAlpha, propBeta);
            double propLogPost = logPosterior(dataset, proposal);
            double acceptProb = Math.min(1.0, Math.exp(propLogPost - currentLogPost));
            if (rng.nextDouble() < acceptProb) {
                current = proposal;
                currentLogPost = propLogPost;
            }

            iter++;
            if (iter > burn && (iter - burn) % thin == 0) {
                samples.add(current);
                kept++;
            }
        }
        return samples;
    }

    private static double clamp01(double x) {
        if (x < 0.0) return 0.0;
        if (x > 1.0) return 1.0;
        return x;
    }
}
