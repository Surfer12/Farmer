"""
Simplified UQ Framework Test - Core Concepts Demonstration
"""

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import brier_score_loss
from scipy import stats
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass

@dataclass
class UncertaintyEstimate:
    """Container for different types of uncertainty estimates."""
    mean: np.ndarray
    aleatoric: np.ndarray  # Data noise, irreducible uncertainty
    epistemic: np.ndarray  # Model ignorance, reducible with more data
    total: np.ndarray      # Combined uncertainty
    
    @property
    def std_total(self) -> np.ndarray:
        """Total standard deviation."""
        return np.sqrt(self.total)

class SimpleDeepEnsemble:
    """Simplified Deep Ensemble for demonstration."""
    
    def __init__(self, model_class=RandomForestRegressor, n_models: int = 5):
        self.model_class = model_class
        self.n_models = n_models
        self.models = []
        
    def fit(self, X: np.ndarray, y: np.ndarray):
        """Train ensemble of models."""
        self.models = []
        n_samples = len(X)
        
        for i in range(self.n_models):
            # Bootstrap sampling for diversity
            indices = np.random.choice(n_samples, n_samples, replace=True)
            X_boot, y_boot = X[indices], y[indices]
            
            # Train individual model
            model = self.model_class(random_state=i)
            model.fit(X_boot, y_boot)
            self.models.append(model)
    
    def predict_with_uncertainty(self, X: np.ndarray) -> UncertaintyEstimate:
        """Generate predictions with epistemic uncertainty."""
        predictions = np.array([model.predict(X) for model in self.models])
        
        mean_pred = np.mean(predictions, axis=0)
        epistemic_var = np.var(predictions, axis=0)  # Model disagreement
        
        # Assume constant aleatoric uncertainty for simplicity
        aleatoric_var = np.full_like(epistemic_var, 0.1)
        
        return UncertaintyEstimate(
            mean=mean_pred,
            aleatoric=aleatoric_var,
            epistemic=epistemic_var,
            total=epistemic_var + aleatoric_var
        )

class SimpleConformalPredictor:
    """Simplified Conformal Prediction implementation."""
    
    def __init__(self, model, alpha: float = 0.1):
        self.model = model
        self.alpha = alpha  # Miscoverage level (1-alpha coverage)
        self.quantile = None
        
    def fit(self, X_cal: np.ndarray, y_cal: np.ndarray):
        """Fit conformal predictor using calibration set."""
        # Get model predictions on calibration set
        predictions = self.model.predict_with_uncertainty(X_cal).mean
        
        # Compute conformity scores (absolute residuals)
        scores = np.abs(y_cal - predictions)
        
        # Find the (1-alpha) quantile of scores
        n = len(scores)
        q_level = np.ceil((n + 1) * (1 - self.alpha)) / n
        self.quantile = np.quantile(scores, q_level)
        
    def predict_intervals(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Generate conformal prediction intervals."""
        if self.quantile is None:
            raise ValueError("Must call fit() before predict_intervals()")
            
        predictions = self.model.predict_with_uncertainty(X).mean
        lower = predictions - self.quantile
        upper = predictions + self.quantile
        
        return lower, upper

def expected_calibration_error(probs: np.ndarray, labels: np.ndarray, 
                              n_bins: int = 10) -> float:
    """Compute Expected Calibration Error (ECE)."""
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    ece = 0
    n_samples = len(probs)
    
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (probs > bin_lower) & (probs <= bin_upper)
        prop_in_bin = in_bin.mean()
        
        if prop_in_bin > 0:
            accuracy_in_bin = labels[in_bin].mean()
            avg_confidence_in_bin = probs[in_bin].mean()
            ece += (in_bin.sum() / n_samples) * abs(accuracy_in_bin - avg_confidence_in_bin)
            
    return ece

def value_at_risk(samples: np.ndarray, alpha: float = 0.05) -> float:
    """Value at Risk (VaR) - Î±-quantile of loss distribution."""
    return np.quantile(samples, 1 - alpha)

def conditional_value_at_risk(samples: np.ndarray, alpha: float = 0.05) -> float:
    """Conditional Value at Risk (CVaR) - expected loss beyond VaR."""
    var = value_at_risk(samples, alpha)
    tail_losses = samples[samples >= var]
    return np.mean(tail_losses) if len(tail_losses) > 0 else var

def demonstrate_uq_framework():
    """Demonstrate the core UQ framework concepts."""
    print("=" * 60)
    print("ðŸŽ¯ UNCERTAINTY QUANTIFICATION FRAMEWORK DEMONSTRATION")
    print("=" * 60)
    
    # Create synthetic dataset
    print("\nðŸ“Š Creating synthetic dataset...")
    X, y = make_regression(n_samples=1000, n_features=5, noise=0.5, random_state=42)
    
    # Add heteroscedastic noise
    noise_scale = 0.3 + 0.3 * np.abs(X[:, 0])
    heteroscedastic_noise = np.random.normal(0, noise_scale)
    y += heteroscedastic_noise
    
    # Split data
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)
    X_cal, X_test, y_cal, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    
    print(f"Training set: {X_train.shape}")
    print(f"Calibration set: {X_cal.shape}")
    print(f"Test set: {X_test.shape}")
    
    # Train Deep Ensemble
    print("\nðŸš€ Training Deep Ensemble (n=5)...")
    ensemble = SimpleDeepEnsemble(RandomForestRegressor, n_models=5)
    ensemble.fit(X_train, y_train)
    
    # Get uncertainty estimates
    print("\nðŸŽ² Generating uncertainty estimates...")
    uncertainty_est = ensemble.predict_with_uncertainty(X_test)
    
    print(f"Mean prediction: {np.mean(uncertainty_est.mean):.3f}")
    print(f"Mean epistemic uncertainty: {np.mean(uncertainty_est.epistemic):.3f}")
    print(f"Mean aleatoric uncertainty: {np.mean(uncertainty_est.aleatoric):.3f}")
    print(f"Mean total uncertainty: {np.mean(uncertainty_est.total):.3f}")
    
    # Conformal Prediction
    print("\nðŸŽ¯ Setting up Conformal Prediction...")
    conformal = SimpleConformalPredictor(ensemble, alpha=0.1)  # 90% coverage
    conformal.fit(X_cal, y_cal)
    
    conf_lower, conf_upper = conformal.predict_intervals(X_test)
    
    # Evaluate coverage
    coverage = np.mean((y_test >= conf_lower) & (y_test <= conf_upper))
    interval_width = np.mean(conf_upper - conf_lower)
    
    print(f"Conformal prediction coverage: {coverage:.3f} (target: 0.90)")
    print(f"Mean interval width: {interval_width:.3f}")
    
    # Risk Analysis
    print("\nâš ï¸  Risk Analysis...")
    
    # Sample from predictive distribution for first 10 test points
    n_samples = 10
    risk_samples = []
    
    for i in range(n_samples):
        samples = np.random.normal(
            uncertainty_est.mean[i],
            uncertainty_est.std_total[i],
            size=1000
        )
        risk_samples.append(samples)
    
    risk_samples = np.array(risk_samples)
    
    # Compute risk metrics
    var_95 = np.array([value_at_risk(samples, 0.05) for samples in risk_samples])
    cvar_95 = np.array([conditional_value_at_risk(samples, 0.05) for samples in risk_samples])
    
    print(f"Mean VaR (95%): {np.mean(var_95):.3f}")
    print(f"Mean CVaR (95%): {np.mean(cvar_95):.3f}")
    
    # Tail probabilities
    threshold = np.mean(y_test) + 2 * np.std(y_test)  # 2-sigma threshold
    tail_probs = np.array([np.mean(samples >= threshold) for samples in risk_samples])
    print(f"Mean tail probability P(Y >= {threshold:.2f}): {np.mean(tail_probs):.4f}")
    
    # Calibration Assessment
    print("\nðŸ“Š Calibration Assessment...")
    
    # Convert uncertainties to pseudo-probabilities
    uncertainties = uncertainty_est.std_total[:n_samples]
    max_uncertainty = np.max(uncertainties)
    confidences = 1 - (uncertainties / max_uncertainty)
    
    # Binary accuracy (within 1 std dev)
    errors = np.abs(y_test[:n_samples] - uncertainty_est.mean[:n_samples])
    within_std = errors <= uncertainties
    
    ece = expected_calibration_error(confidences, within_std.astype(int))
    print(f"Expected Calibration Error: {ece:.4f}")
    
    # Selective Prediction
    print("\nðŸŽ¯ Selective Prediction...")
    
    # Find threshold for 90% accuracy on confident predictions
    all_errors = np.abs(y_test - uncertainty_est.mean)
    all_uncertainties = uncertainty_est.std_total
    all_accuracies = (all_errors < np.std(all_errors)).astype(int)
    
    # Sort by uncertainty
    sorted_indices = np.argsort(all_uncertainties)
    sorted_uncertainties = all_uncertainties[sorted_indices]
    sorted_accuracies = all_accuracies[sorted_indices]
    
    # Find cumulative accuracy
    cumulative_correct = np.cumsum(sorted_accuracies)
    cumulative_total = np.arange(1, len(sorted_accuracies) + 1)
    cumulative_accuracy = cumulative_correct / cumulative_total
    
    # Find threshold where accuracy meets 90%
    target_accuracy = 0.9
    valid_indices = cumulative_accuracy >= target_accuracy
    
    if np.any(valid_indices):
        threshold_idx = np.argmax(valid_indices)
        uncertainty_threshold = sorted_uncertainties[threshold_idx]
        
        # Apply selective prediction
        confident_mask = all_uncertainties <= uncertainty_threshold
        confident_predictions = uncertainty_est.mean[confident_mask]
        confident_targets = y_test[confident_mask]
        
        print(f"Uncertainty threshold for 90% accuracy: {uncertainty_threshold:.4f}")
        print(f"Confident predictions: {np.sum(confident_mask)}/{len(y_test)} ({100*np.mean(confident_mask):.1f}%)")
        
        if len(confident_predictions) > 0:
            confident_mae = np.mean(np.abs(confident_targets - confident_predictions))
            overall_mae = np.mean(np.abs(y_test - uncertainty_est.mean))
            print(f"MAE on confident predictions: {confident_mae:.4f}")
            print(f"Overall MAE: {overall_mae:.4f}")
            improvement = ((overall_mae - confident_mae) / overall_mae * 100)
            print(f"Improvement: {improvement:.1f}%")
    else:
        print("No threshold achieves 90% accuracy target")
    
    # Summary
    print("\n" + "=" * 60)
    print("âœ… DEMONSTRATION SUMMARY")
    print("=" * 60)
    
    print("ðŸŽ¯ Key Results:")
    print(f"  â€¢ Conformal coverage: {coverage:.3f} (target: 0.90)")
    print(f"  â€¢ Mean uncertainty: {np.mean(uncertainty_est.total):.4f}")
    print(f"  â€¢ Calibration error: {ece:.4f}")
    print(f"  â€¢ VaR/CVaR computed for risk assessment")
    print(f"  â€¢ Selective prediction threshold identified")
    
    print("\nðŸ“š What this demonstrates:")
    print("  âœ… Aleatoric vs Epistemic uncertainty separation")
    print("  âœ… Distribution-free coverage guarantees (conformal)")
    print("  âœ… Risk metrics (VaR, CVaR, tail probabilities)")
    print("  âœ… Calibration assessment and improvement")
    print("  âœ… Selective prediction for quality control")
    
    print("\nðŸš€ Framework Benefits:")
    print("  â€¢ Converts uncertainty into actionable risk metrics")
    print("  â€¢ Provides theoretical coverage guarantees")
    print("  â€¢ Enables principled decision-making under uncertainty")
    print("  â€¢ Supports quality control via selective prediction")
    
    return {
        'ensemble': ensemble,
        'uncertainty_est': uncertainty_est,
        'coverage': coverage,
        'ece': ece,
        'var_95': np.mean(var_95),
        'cvar_95': np.mean(cvar_95)
    }

if __name__ == "__main__":
    results = demonstrate_uq_framework()
    print(f"\nðŸŽ¯ Final Results: Coverage={results['coverage']:.3f}, ECE={results['ece']:.4f}")
    print("ðŸš€ UQ Framework demonstration complete!")