# Uncertainty Quantification for Reliable Risk Estimation

A production-ready framework for uncertainty quantification (UQ) that separates epistemic from aleatoric uncertainty, provides calibrated predictions, and enables risk-aware decision making.

## ğŸ¯ Key Benefits

- **Separates Ignorance from Noise**: Distinguishes epistemic (model) uncertainty from aleatoric (data) uncertainty
- **Actionable Risk Outputs**: Provides tail probabilities, confidence intervals, and abstention triggers
- **Calibrated Predictions**: Ensures predicted probabilities match observed frequencies
- **Risk-Aware Decisions**: Implements expected cost minimization and tail risk management

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run basic example
python examples/quick_start.py

# Run comprehensive demo
jupyter notebook notebooks/uq_demo.ipynb
```

## ğŸ“Š Core Methods

### Deep Ensembles
- Strong baseline for epistemic uncertainty (nâ‰ˆ5 models)
- Simple to implement, excellent performance

### Lightweight Bayesian
- MC Dropout: Uncertainty from dropout at inference
- SWAG: Stochastic Weight Averaging Gaussian
- Laplace Approximation: Last-layer Bayesian treatment

### Heteroscedastic/Quantile Regression
- Predict mean + variance directly
- Quantile regression for distribution-free intervals

### Conformal Prediction
- Distribution-free coverage guarantees
- Works with any base predictor

## ğŸ“ˆ Evaluation Metrics

- **Calibration**: ECE, ACE, reliability diagrams, Brier score
- **Intervals**: Coverage (PICP), width (MPIW), CRPS
- **OOD Detection**: AUROC, FPR@95%TPR
- **Decision Quality**: Expected cost, VaR, CVaR

## ğŸ›ï¸ Risk Decision Framework

```python
# Minimal decision pseudocode
if uncertainty > threshold or conformal_set_size > max_size:
    action = "abstain_or_escalate"
else:
    action = argmin_a(expected_cost(a, p_y_given_x))
```

## ğŸ“ Project Structure

```
uncertainty_quantification/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # UQ model implementations
â”‚   â”œâ”€â”€ calibration/     # Calibration methods
â”‚   â”œâ”€â”€ conformal/       # Conformal prediction
â”‚   â”œâ”€â”€ metrics/         # Evaluation metrics
â”‚   â”œâ”€â”€ decisions/       # Risk-aware decision making
â”‚   â””â”€â”€ monitoring/      # Drift detection and monitoring
â”œâ”€â”€ examples/            # Quick examples
â”œâ”€â”€ notebooks/           # Jupyter notebooks
â””â”€â”€ tests/              # Unit tests
```

## ğŸ¯ Performance Targets

- ECE â‰¤ 2-3% on in-domain data
- Conformal coverage within Â±1-2% of nominal
- OOD FPR@95%TPR substantially below baseline
- Improved expected cost and lower tail losses

## ğŸ“š References

See `notebooks/uq_demo.ipynb` for detailed examples and explanations.